{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras\n",
        "!pip install split-folders # install the package to split the images in TRAIN and TEST folders\n",
        "!pip install ann_visualizer # visualize NN architectures"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWOrFE7O46B1",
        "outputId": "cab3ed64-0131-467d-fd6c-1c34e9409277"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.9.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: importlib-metadata>=3 in /usr/local/lib/python3.7/dist-packages (from scikeras) (4.13.0)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.7/dist-packages (from scikeras) (21.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikeras) (1.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=0.21->scikeras) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.7.3)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.9.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ann_visualizer\n",
            "  Downloading ann_visualizer-2.5.tar.gz (4.7 kB)\n",
            "Building wheels for collected packages: ann-visualizer\n",
            "  Building wheel for ann-visualizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ann-visualizer: filename=ann_visualizer-2.5-py3-none-any.whl size=4168 sha256=c7c1e1334b91e12cdc56c9879f3c6e49558af2d4d9f49292b4a2669f2c6211d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/fc/58/2ab1c3b30350105929308becddda4fb59b1358e54f985e1f4a\n",
            "Successfully built ann-visualizer\n",
            "Installing collected packages: ann-visualizer\n",
            "Successfully installed ann-visualizer-2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sVZbgW2F22Cf"
      },
      "outputs": [],
      "source": [
        "# IMPORT TENSORFLOW/KERAS\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
        "\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Activation\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.pooling import GlobalMaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "# TRANSFER LEARNING\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "\n",
        "# SCIKIT-LEARN/SCIKERAS\n",
        "from sklearn.decomposition import PCA\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# IMPORT OTHER MODULES\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import fnmatch # to count number of image per folder easily\n",
        "import random # to select random images from a folder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import splitfolders # needed to split the images in TRAIN and TEST folders\n",
        "from os import mkdir\n",
        "from pathlib import Path\n",
        "from shutil import copyfile # to import \"copyfile\"\n",
        "import plotly.figure_factory as ff # for printing the heatmap\n",
        "from collections import defaultdict\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.image as img_mat\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# CV2\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# USE THE FOLLOWING RESOURCES\n",
        "\n",
        "**VIDEOS**\n",
        "- https://www.youtube.com/watch?v=kHtToZidh3A\n",
        "\n",
        "GITHUB IMPLEMENTATION\n",
        "- https://github.com/bnsreenu/python_for_microscopists/blob/master/176-multiclass_using_VGG_weights_PCA_NN_RF.py"
      ],
      "metadata": {
        "id": "ZUm6TdrZP7vM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the notebook TRAINING e aggiungici anche questa parte di PCA all'inizio quando carichi tutte le immagini. Poi ci applichi i modelli. \n",
        "APplicare anche il datetime per il clcolo di ogni cella di modello.\n",
        "Applicare anche quanto visto qua:\n",
        "- https://shankarmsy.github.io/posts/pca-sklearn.html\n",
        "- https://towardsdatascience.com/rgb-color-image-compression-using-principal-component-analysis-fce3f48dfdd0\n",
        "- https://towardsdatascience.com/using-pca-to-reduce-number-of-parameters-in-a-neural-network-by-30x-times-fcc737159282\n",
        "\n",
        "Unicamente per mostrare l'immagine finale sotto diverse applicazioni della PCA (con 400, 300, 200, 100, 50, 10 dimensioni), con il codice seguente:"
      ],
      "metadata": {
        "id": "1154dbLuQEn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of the learning curves\n",
        "plt.figure(figsize=(15,6))\n",
        "\n",
        "subplot_index = 1\n",
        "for n_components in [400, 300, 200, 100, 50, 10]:\n",
        "    \n",
        "    plt.subplot(3, 2, subplot_index)\n",
        "    subplot_index = subplot_index + 1\n",
        "\n",
        "    ipca = PCA(n_components).fit(img_reshaped)\n",
        "    transf_img = ipca.transform(img_reshaped)\n",
        "    print(\"transf_img shape is:\", transf_img.shape)\n",
        "\n",
        "    print(\"-----------------------------------\")\n",
        "    print(f\"Explained variances for {n_components}:\", np.sum(ipca.explained_variance_ratio_ ) * 100)\n",
        "    print(\"-----------------------------------\")\n",
        "\n",
        "    # restore the image from the subspace\n",
        "    image_restored = ipca.inverse_transform(transf_img)\n",
        "\n",
        "    # reshape the image to the original array size\n",
        "    image_restored = np.reshape(image_restored, (np.size(img_data, 0),\n",
        "                                                 np.size(img_data, 1),\n",
        "                                                 np.size(img_data, 2)))\n",
        "    \n",
        "    image_restored = image_restored.astype(np.uint8)\n",
        "    # cv2.imwrite(f\"/content/{n_components}.jpg\", cv2.cvtColor(image_restored, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.title('n_components:' + str(n_components))\n",
        "    plt.imshow(image_restored)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yfvi4ILj6Z1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/100.jpg')"
      ],
      "metadata": {
        "id": "Nuk0PeF66Zzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------"
      ],
      "metadata": {
        "id": "8wbIz7hqiuR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Link Google Drive account\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WXvue_-jKp0",
        "outputId": "f08151d6-e50c-43c3-e98a-939429445290"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Insert the directory\n",
        "import sys\n",
        "sys.path.insert(0,\"/content/gdrive/MyDrive/Data_Science_2020-2022/Secondo_anno_Secondo_Semestre/FoDL_Project/Project_Example_Food/train_test\")"
      ],
      "metadata": {
        "id": "eXcknEO4jvcy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONSTANT\n",
        "# SCALE_PERCENT = 6 # resizing percentage\n",
        "SEED_VALUE = 42\n",
        "num_classes = 9 # number of output classes\n",
        "\n",
        "# PATH TO THE DATA\n",
        "DIR_TRAIN_DATA = '/content/gdrive/MyDrive/Data_Science_2020-2022/Secondo_anno_Secondo_Semestre/FoDL_Project/Project_Example_Food/train_test/train/*' # path contenente i sottoinsiemi di train, test\n",
        "DIR_TEST_DATA = '/content/gdrive/MyDrive/Data_Science_2020-2022/Secondo_anno_Secondo_Semestre/FoDL_Project/Project_Example_Food/train_test/test/*' # path contenente i sottoinsiemi di train, test"
      ],
      "metadata": {
        "id": "jRiyHXhHj36w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for directory_path in glob.glob(DIR_TRAIN_DATA):\n",
        "    label = directory_path.split(\"/\")[-1]\n",
        "    print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc_jVx57kknc",
        "outputId": "62ef051d-22c6-46a5-9ebb-24c7207127f2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "french_fries\n",
            "caprese_salad\n",
            "pizza\n",
            "greek_salad\n",
            "sashimi\n",
            "hot_dog\n",
            "caesar_salad\n",
            "hamburger\n",
            "sushi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SIZE = 224  #Resize images\n",
        "\n",
        "#Capture training data and labels into respective lists\n",
        "train_images = []\n",
        "train_labels = [] \n",
        "\n",
        "# GET THE LABELS\n",
        "for directory_path in glob.glob(DIR_TRAIN_DATA):\n",
        "    tr_label = directory_path.split(\"/\")[-1]\n",
        "    print(label)\n",
        "    # GET THE IMAGES\n",
        "    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
        "        print(img_path)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
        "        img = cv2.resize(img, (SIZE, SIZE))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        train_images.append(img)\n",
        "        train_labels.append(tr_label)\n",
        "\n",
        "# CONVERT LISTS TO ARRAYS\n",
        "train_images = np.array(train_images)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# CAPTURE TEST/VALIDATION DATA AND LABELS INTO RESPECTIVE LISTS\n",
        "\n",
        "test_images = []\n",
        "test_labels = [] \n",
        "for directory_path in glob.glob(DIR_TEST_DATA):\n",
        "    tt_label = directory_path.split(\"/\")[-1]\n",
        "    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        img = cv2.resize(img, (SIZE, SIZE))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        test_images.append(img)\n",
        "        test_labels.append(tt_label)\n",
        "\n",
        "#Convert lists to arrays                \n",
        "test_images = np.array(test_images)\n",
        "test_labels = np.array(test_labels)"
      ],
      "metadata": {
        "id": "Gw70nYVWiuIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of training images:\", len(train_images))\n",
        "print(\"Length of training labels:\", len(train_labels))\n",
        "print()\n",
        "print(\"Length of training images:\", len(test_images))\n",
        "print(\"Length of training labels:\", len(test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izhTbR3fqAXJ",
        "outputId": "33446d72-5554-41ea-aade-ad204a93645e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of training images: 1803\n",
            "Length of training labels: 1803\n",
            "\n",
            "Length of training images: 155\n",
            "Length of training labels: 155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONTINUARE DA QUI!"
      ],
      "metadata": {
        "id": "Pq3b-1J5ql1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encode labels from text to integers.\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(test_labels)\n",
        "test_labels_encoded = le.transform(test_labels)\n",
        "le.fit(train_labels)\n",
        "train_labels_encoded = le.transform(train_labels)\n",
        "\n",
        "#Split data into test and train datasets (already split but assigning to meaningful convention)\n",
        "x_train, y_train, x_test, y_test = train_images, train_labels_encoded, test_images, test_labels_encoded\n",
        "\n",
        "###################################################################\n",
        "# Scale pixel values to between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "#One hot encode y values for neural network. Not needed for Random Forest\n",
        "from keras.utils import to_categorical\n",
        "y_train_one_hot = to_categorical(y_train)\n",
        "y_test_one_hot = to_categorical(y_test)\n",
        "\n",
        "#############################"
      ],
      "metadata": {
        "id": "pcwNNp_AiuGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PC0B4fzyiuD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lkBJq-6aiuBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CHeqf__0it_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HQmvK7mHit9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "REojPc0Ait6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HXuYbkFjit4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kVd0R_1Vit12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BgYsHWiuitzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJtol1Y36ZtJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}